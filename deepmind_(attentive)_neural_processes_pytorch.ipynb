{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replication of Deepmind's official NP/ANPs implementation in Pytorch\n",
    "\n",
    "## Differences between Deemind's CNP and ANP implementations\n",
    "- Data Generator\n",
    "  - lengthscale is 0.6 here\n",
    "  - random_kernel_parameters = True: kernel parameters are sampled: samples unformly between 0.1 and 0.6 for lengthscale. Vary across batches even.\n",
    "\n",
    "- batch_mlp function:\n",
    "- \n",
    "\n",
    "## Resources\n",
    "\n",
    "**Conditional Neural Processes**:   \n",
    "Garnelo M, Rosenbaum D, Maddison CJ, Ramalho T, Saxton D, Shanahan M, Teh YW, Rezende DJ, Eslami SM. Conditional Neural Processes. In International Conference on Machine Learning 2018.\n",
    "\n",
    "**Neural Processes**:  \n",
    "Garnelo, M., Schwarz, J., Rosenbaum, D., Viola, F., Rezende, D.J., Eslami, S.M. and Teh, Y.W. Neural processes. ICML Workshop on Theoretical Foundations and Applications of Deep Generative Models 2018.\n",
    "\n",
    "**Attentive Neural Processes**:   \n",
    "Kim, H., Mnih, A., Schwarz, J., Garnelo, M., Eslami, A., Rosenbaum, D., Vinyals, O. and Teh, Y.W. Attentive Neural Processes. In International Conference on Learning Representations 2019.\n",
    "\n",
    "[Link to Deepmind's Google Colab (A)NP ](https://colab.research.google.com/github/deepmind/neural-processes/blob/master/attentive_neural_process.ipynb)  \n",
    "[Link to Deemind's Neural Processes GitHub repository](https://github.com/deepmind/neural-processes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CNP takes as input a `CNPRegressionDescription` namedtuple with fields:\n",
    "#   `query`: a tuple containing ((context_x, context_y), target_x)\n",
    "#   `target_y`: a tesor containing the ground truth for the targets to be\n",
    "#     predicted\n",
    "#   `num_total_points`: A vector containing a scalar that describes the total\n",
    "#     number of datapoints used (context + target)\n",
    "#   `num_context_points`: A vector containing a scalar that describes the number\n",
    "#     of datapoints used as context\n",
    "# The GPCurvesReader returns the newly sampled data in this format at each\n",
    "# iteration\n",
    "\n",
    "CNPRegressionDescription = collections.namedtuple(\n",
    "    \"CNPRegressionDescription\",\n",
    "    (\"query\", \"target_y\", \"num_total_points\", \"num_context_points\"))\n",
    "\n",
    "class GPCurvesReader(object):\n",
    "  \"\"\"Generates curves using a Gaussian Process (GP).\n",
    "\n",
    "  Supports vector inputs (x) and vector outputs (y). Kernel is\n",
    "  mean-squared exponential, using the x-value l2 coordinate distance scaled by\n",
    "  some factor chosen randomly in a range. Outputs are independent gaussian\n",
    "  processes.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               batch_size,\n",
    "               max_num_context,\n",
    "               x_size = 1,\n",
    "               y_size = 1,\n",
    "               l1_scale = 0.6,\n",
    "               sigma_scale = 1.0,\n",
    "               random_kernel_parameters=True\n",
    "               testing = False):\n",
    "    \"\"\"Creates a regression dataset of functions sampled from a GP.\n",
    "\n",
    "    Args:\n",
    "      batch_size: An integer.\n",
    "      max_num_context: The max number of observations in the context.\n",
    "      x_size: Integer >= 1 for length of \"x values\" vector.\n",
    "      y_size: Integer >= 1 for length of \"y values\" vector.\n",
    "      l1_scale: Float; typical scale for kernel distance function.\n",
    "      sigma_scale: Float; typical scale for variance.\n",
    "      random_kernel_parameters: If `True`, the kernel parameters (l1 and sigma) \n",
    "          will be sampled uniformly within [0.1, l1_scale] and [0.1, sigma_scale].\n",
    "      testing: Boolean that indicates whether we are testing. If so there are\n",
    "          more targets for visualization.\n",
    "    \"\"\"\n",
    "    self._batch_size = batch_size\n",
    "    self._max_num_context = max_num_context\n",
    "    self._x_size = x_size\n",
    "    self._y_size = y_size\n",
    "    self._l1_scale = l1_scale\n",
    "    self._sigma_scale = sigma_scale\n",
    "    self._random_kernel_parameters = random_kernel_parameters\n",
    "    self._testing = testing\n",
    "\n",
    "  def _gaussian_kernel(self, xdata, l1, sigma_f, sigma_noise = 2e-2):\n",
    "    \"\"\"Applies the Gaussian kernel to generate curve data.\n",
    "\n",
    "    Args:\n",
    "      xdata: Tensor with shape `[batch_size, num_total_points, x_size]` with\n",
    "          the values of the x-axis data.\n",
    "      l1: Tensor with shape `[batch_size, y_size, x_size]`, the scale\n",
    "          parameter of the Gaussian kernel.\n",
    "      sigma_f: Float tensor with shape `[batch_size, y_size]`; the magnitude\n",
    "          of the std.\n",
    "      sigma_noise: Float, std of the noise that we add for stability.\n",
    "\n",
    "    Returns:\n",
    "      The kernel, a float tensor with shape\n",
    "      `[batch_size, y_size, num_total_points, num_total_points]`.\n",
    "    \"\"\"\n",
    "    # Extract number of data points from tensor so this works for both testing and training\n",
    "    num_total_points = xdata.shape[1]\n",
    "    # num_total_points = tf.shape(xdata)[1]\n",
    "\n",
    "    # Expand and take the difference\n",
    "    xdata1 = xdata.unsqueeze(dim = 1) # [B, 1, num_total_points, x_size]\n",
    "    xdata2 = xdata.unsqueeze(dim = 2) # [B, num_total_points, 1, x_size]\n",
    "    # xdata1 = tf.expand_dims(xdata, axis=1)  # [B, 1, num_total_points, x_size]\n",
    "    # xdata2 = tf.expand_dims(xdata, axis=2)  # [B, num_total_points, 1, x_size]\n",
    "\n",
    "    diff = xdata1 - xdata2  # [B, num_total_points, num_total_points, x_size]\n",
    "\n",
    "    # Insert dimension for y_size: [B, y_size, num_total_points, num_total_points, x_size]\n",
    "    # same as diff[:, None, :, :, :]\n",
    "    diff_expanded = diff.unsqueeze(dim = 1)\n",
    "\n",
    "    # Scale the differences (lengthscale) and square\n",
    "    # l1[:, :, None, None, :] created explicit dimensions to that dimensionality matches\n",
    "    norm = torch.square(diff_expanded / l1[:, :, None, None, :])\n",
    "    # Norm has shape [B, y_size, num_total_points, num_total_points, x_size]\n",
    "    # norm = tf.square(diff[:, None, :, :, :] / l1[:, :, None, None, :])\n",
    "\n",
    "    # Sum along last dimension (x_size) to reduce this dimension\n",
    "    norm = torch.sum(norm, dim = -1)\n",
    "    # Norm now has shape [B, y_size, num_total_points, num_total_points]\n",
    "\n",
    "    # norm = tf.reduce_sum(norm, -1)  # [B, data_size, num_total_points, num_total_points]\n",
    "\n",
    "    kernel = torch.square(sigma_f)[:, :, None, None] * torch.exp(-0.5 * norm)\n",
    "    # kernel = tf.square(sigma_f)[:, :, None, None] * tf.exp(-0.5 * norm)\n",
    "    # Kernel has shape [B, y_size, num_total_points, num_total_points]\n",
    "\n",
    "    # Add some noise to the diagonal to make the cholesky work.\n",
    "    # sigma_noise.pow(2) = sigma_noise ** 2\n",
    "    kernel = kernel + ((sigma_noise ** 2) * torch.eye(n = num_total_points))\n",
    "    # kernel += (sigma_noise**2) * tf.eye(num_total_points)\n",
    "\n",
    "    return kernel\n",
    "\n",
    "  def generate_curves(self):\n",
    "    \"\"\"Builds the op delivering the data.\n",
    "\n",
    "    Generated functions are `float32` with x values between -2 and 2.\n",
    "    \n",
    "    Returns:\n",
    "      A `CNPRegressionDescription` namedtuple.\n",
    "    \"\"\"\n",
    "    # Sample number of context points between 3 and max_num_content\n",
    "    # Torch: low (inclusive) and high (exclusive)\n",
    "    num_context = torch.randint(low = 3, high = self._max_num_context, size = (1,))\n",
    "    # num_context = tf.random_uniform(shape = [], minval = 3, maxval = self._max_num_context, dtype = tf.int32)\n",
    "\n",
    "    ### X-VALUES ###\n",
    "    # If we are TESTING we want to have more targets and have them evenly distributed in order to plot the function.\n",
    "    if self._testing:\n",
    "      num_target = 400\n",
    "      num_total_points = num_target\n",
    "      # tf.expand_dims or torch.unsqueeze add dimension of length one\n",
    "      # torch.tile create x batch replicas\n",
    "      x_values = torch.tile(input = torch.arange(start = -2., end = 2., step = 1./100).unsqueeze(dim = 0),\n",
    "                 dims = (self._batch_size, 1))\n",
    "     \n",
    "      # Add explicit last dimension\n",
    "      x_values = x_values.unsqueeze(dim = -1)\n",
    "      # x_value has shape (batch_size, num_target, x_size)\n",
    "\n",
    "      # x_values = tf.tile(tf.expand_dims(tf.range(-2., 2. , 1. / 100, dtype = tf.float32), axis = 0),[self._batch_size, 1])\n",
    "      # x_values = tf.expand_dims(x_values, axis=-1)\n",
    "\n",
    "    # During TRAINING the number of target points and their x-positions are selected at random\n",
    "    # Since x_value samples have no order, the kernel looks funky\n",
    "    else:\n",
    "      num_target = torch.randint(low = 2, high = self._max_num_context, size = (1,))\n",
    "      # num_target = tf.random_uniform(shape = (), minval = 2, maxval = self._max_num_context, dtype = tf.int32)\n",
    "      num_total_points = num_context + num_target\n",
    "      # sample unformly between [0, 1), then scale and shift\n",
    "      x_values = ((torch.rand(size = (self._batch_size, num_total_points, self._x_size)) * 4) - 2)\n",
    "      # x_values = tf.random_uniform([self._batch_size, num_total_points, self._x_size], -2, 2)\n",
    "\n",
    "    ### Y-VALUES ###\n",
    "    if self._random_kernel_parameters:\n",
    "      l1 = ((torch.rand(size = (self._batch_size, self._y_size, self._x_size)) * (self._l1_scale - 0.1)) + 0.1)\n",
    "      sigma_f = ((torch.rand(size = (self._batch_size, self._y_size, self._x_size)) * (self._sigma_scale - 0.1)) + 0.1)\n",
    "    # Set kernel parameters\n",
    "    # Copy l1_scale and sigma_f into the right shaped tensors\n",
    "\n",
    "    # Same Fixed parameters\n",
    "    else:\n",
    "      l1 = torch.ones(size = (self._batch_size, self._y_size, self._x_size)) * self._l1_scale\n",
    "      sigma_f = torch.ones(size = (self._batch_size, self._y_size)) * self._sigma_scale\n",
    "\n",
    "    # l1 = (tf.ones(shape = [self._batch_size, self._y_size, self._x_size]) * self._l1_scale)\n",
    "    # sigma_f = tf.ones(shape = [self._batch_size, self._y_size]) * self._sigma_scale\n",
    "\n",
    "    ### GAUSSIAN KERNEL ###\n",
    "    # Pass the x_values through the Gaussian kernel\n",
    "    # [batch_size, y_size, num_total_points, num_total_points]\n",
    "    kernel = self._gaussian_kernel(x_values, l1, sigma_f)\n",
    "\n",
    "    # Computes the Cholesky decomposition for batches of symmetric positive-definite matrices\n",
    "    cholesky = torch.linalg.cholesky(kernel)\n",
    "    # cholesky has shape [batch_size, y_size, num_total_points, num_total_points]\n",
    "\n",
    "    # Calculate Cholesky, using double precision for better stability:\n",
    "    # cholesky = tf.cast(tf.cholesky(tf.cast(kernel, tf.float64)), tf.float32)\n",
    "\n",
    "    # Sample a curve: randn stand for random normal\n",
    "    y_values = torch.matmul(cholesky, torch.randn(size = (self._batch_size, self._y_size, num_total_points, 1)))\n",
    "    # y_values has shape [batch_size, y_size, num_total_points, 1]\n",
    "\n",
    "    # y_values = tf.matmul(cholesky, tf.random_normal([self._batch_size, self._y_size, num_total_points, 1]))\n",
    "\n",
    "    # Squeeze last dimension and transpose last two dimensions\n",
    "    y_values = torch.transpose(input = y_values.squeeze(dim = -1), dim0 = 2, dim1 = 1)\n",
    "    # y_values now has shape [batch_size, num_total_points, y_size]\n",
    "    \n",
    "    # y_values = tf.transpose(tf.squeeze(y_values, 3), [0, 2, 1])\n",
    "\n",
    "    if self._testing:\n",
    "      # Select the targets\n",
    "      target_x = x_values\n",
    "      target_y = y_values\n",
    "\n",
    "      # Select the observations (num_context subset of target)\n",
    "      # Returns a random permutation of integers from 0 to n - 1.\n",
    "      idx = torch.randperm(n = int(num_target))\n",
    "      # idx = tf.random_shuffle(tf.range(num_target))\n",
    "\n",
    "      # Subset first \"num_context\" points from dim 1 into the context\n",
    "      context_x = x_values[:, idx[:num_context], :]\n",
    "      context_y = y_values[:, idx[:num_context], :]\n",
    "\n",
    "      # context_x = tf.gather(x_values, idx[:num_context], axis=1)\n",
    "      # context_y = tf.gather(y_values, idx[:num_context], axis=1)\n",
    "\n",
    "    else:\n",
    "      # Select the targets which will consist of the context points as well as some new target points\n",
    "      # same as target_x = x_values (all values)\n",
    "      target_x = x_values[:, :num_target + num_context, :]\n",
    "      target_y = y_values[:, :num_target + num_context, :]\n",
    "\n",
    "      # Select the observations\n",
    "      context_x = x_values[:, :num_context, :]\n",
    "      context_y = y_values[:, :num_context, :]\n",
    "\n",
    "    query = ((context_x, context_y), target_x)\n",
    "\n",
    "    return CNPRegressionDescription(\n",
    "        query = query,\n",
    "        target_y = target_y,\n",
    "        num_total_points = target_x.shape[1],\n",
    "        # num_total_points=tf.shape(target_x)[1],\n",
    "        num_context_points = num_context)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder: Latent Path:\n",
    "\n",
    "I have left off here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class LatentEncoder(nn.Module):\n",
    "  \"\"\"The Latent Encoder.\"\"\"\n",
    "\n",
    "  def __init__(self, output_sizes, num_latents):\n",
    "    \"\"\"(A)NP latent encoder.\n",
    "\n",
    "    Args:\n",
    "      output_sizes: An iterable containing the output sizes of the encoding MLP.\n",
    "      num_latents: The latent dimensionality.\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self._output_sizes = output_sizes\n",
    "    self._num_latents = num_latents\n",
    "\n",
    "    # PyTorch: need to initiate layers in __init__ not forward()\n",
    "    # First layer - Warning: HARDCODE 2\n",
    "    self.module_list = nn.ModuleList([nn.Linear(in_features = 2, out_features = self._output_sizes[0])])\n",
    "    # Activate\n",
    "    self.module_list.append(nn.ReLU(inplace = True))\n",
    "\n",
    "    # Add as many layers as needed\n",
    "    for i, size in enumerate(self._output_sizes[1 : -1]):\n",
    "      # i: previous index since we start at 1 \n",
    "      self.module_list.append(nn.Linear(in_features = self._output_sizes[i], out_features = self._output_sizes[i + 1]))\n",
    "      self.module_list.append(nn.ReLU(inplace = True))\n",
    "\n",
    "    # Last layer without activation\n",
    "    self.module_list.append(nn.Linear(in_features = self._output_sizes[-2], out_features = self._output_sizes[-2]))\n",
    "\n",
    "  def forward(self, context_x, context_y):\n",
    "    \"\"\"Encodes the inputs into one representation.\n",
    "\n",
    "    Args:\n",
    "      context_x: Tensor of size bs x observations x m_ch. For this 1D regression\n",
    "          task this corresponds to the x-values.\n",
    "      context_y: Tensor of size bs x observations x d_ch. For this 1D regression\n",
    "          task this corresponds to the y-values.\n",
    "      num_context_points: A tensor containing a single scalar that indicates the\n",
    "          number of context_points provided in this iteration.\n",
    "\n",
    "    Returns:\n",
    "      representation: The encoded representation averaged over all context \n",
    "          points.\n",
    "    \"\"\"\n",
    "    # Concatenate x and y along the filter axes\n",
    "    # DIFFERS from other implementation which concats along dim = 1\n",
    "    encoder_input = torch.cat((context_x, context_y), dim = -1)\n",
    "    # encoder_input = tf.concat([context_x, context_y], axis = -1)\n",
    "\n",
    "    # Get the shapes of the input and reshape to parallelise across observations & batches\n",
    "    batch_size, num_context_points , filter_size = encoder_input.shape\n",
    "    # batch_size, _ , filter_size = encoder_input.shape.as_list()\n",
    "\n",
    "    # Combine dim batches to improve parallelisation\n",
    "    hidden = torch.reshape(input = encoder_input, shape = (batch_size * num_context_points, -1))\n",
    "    # hidden = tf.reshape(encoder_input, (batch_size * num_context_points, -1))\n",
    "    # Redundant:\n",
    "    # hidden.set_shape((None, filter_size))\n",
    "\n",
    "    # FORWARD\n",
    "    for module in self.module_list:\n",
    "            hidden = module(hidden)\n",
    "\n",
    "    # # Pass through MLP\n",
    "    # with tf.variable_scope(\"encoder\", reuse=tf.AUTO_REUSE):\n",
    "    #  for i, size in enumerate(self._output_sizes[:-1]):\n",
    "    #    hidden = tf.nn.relu(\n",
    "    #       tf.layers.dense(hidden, size, name=\"Encoder_layer_{}\".format(i)))\n",
    "\n",
    "    #  # Last layer without a ReLu\n",
    "    #  hidden = tf.layers.dense(\n",
    "    #      hidden, self._output_sizes[-1], name = \"Encoder_layer_{}\".format(i + 1))\n",
    "\n",
    "    # Bring back into original shape\n",
    "    hidden = torch.reshape(input = hidden, shape = (batch_size, num_context_points, self._output_sizes[-1]))\n",
    "    # hidden = tf.reshape(hidden, (batch_size, num_context_points, size))\n",
    "\n",
    "    # Aggregator: take the mean over all points (dim 1). One represntation per batch.\n",
    "    representation = torch.mean(input = hidden, dim = 1)\n",
    "    # representation = tf.reduce_mean(hidden, axis=1)\n",
    "\n",
    "    return representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility methods\n",
    "def batch_mlp(input, output_sizes, variable_scope):\n",
    "  \"\"\"Apply MLP to the final axis of a 3D tensor (reusing already defined MLPs).\n",
    "  \n",
    "  Args:\n",
    "    input: input tensor of shape [B,n,d_in].\n",
    "    output_sizes: An iterable containing the output sizes of the MLP as defined \n",
    "        in `basic.Linear`.\n",
    "    variable_scope: String giving the name of the variable scope. If this is set\n",
    "        to be the same as a previously defined MLP, then the weights are reused.\n",
    "    \n",
    "  Returns:\n",
    "    tensor of shape [B, n, d_out] where d_out = output_sizes[-1]\n",
    "  \"\"\"\n",
    "  # Get the shapes of the input and reshape to parallelise across observations\n",
    "  batch_size, _, filter_size = input.shape\n",
    "  # batch_size, _, filter_size = input.shape.as_list()\n",
    "  output = torch.reshape(input = input, shape = (-1, filter_size))\n",
    "  # output = tf.reshape(input, (-1, filter_size))\n",
    "  # Redundant\n",
    "  # output.set_shape((None, filter_size))\n",
    "\n",
    "  # Pass through MLP\n",
    "  with tf.variable_scope(variable_scope, reuse=tf.AUTO_REUSE):\n",
    "    for i, size in enumerate(output_sizes[:-1]):\n",
    "      output = tf.nn.relu(\n",
    "          tf.layers.dense(output, size, name=\"layer_{}\".format(i)))\n",
    "\n",
    "    # Last layer without a ReLu\n",
    "    output = tf.layers.dense(\n",
    "        output, output_sizes[-1], name=\"layer_{}\".format(i + 1))\n",
    "\n",
    "  # Bring back into original shape\n",
    "  output = tf.reshape(output, (batch_size, -1, output_sizes[-1]))\n",
    "  return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9",
   "language": "python",
   "name": "py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
